{
 "metadata": {
  "name": "",
  "signature": "sha256:f4241518d571e5faf59e3f888c2e68022335cfe453e4b7158aba077fe026bbd8"
 },
 "nbformat": 3,
 "nbformat_minor": 0,
 "worksheets": [
  {
   "cells": [
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "%matplotlib inline\n",
      "import codecs\n",
      "import collections\n",
      "import itertools\n",
      "import re\n",
      "import unicodedata\n",
      "\n",
      "import matplotlib.pyplot as plt\n",
      "import matplotlib.patches as patches\n",
      "import matplotlib.path as path\n",
      "\n",
      "import numpy as np"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [],
     "prompt_number": 10
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "First, we'll read in the data from the file. We'll use `codecs.open` to convert it to Unicode as we're going. That will allow us to read in and work with the curly quote characters correctly."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "f = codecs.open('night_and_day.txt', 'r', 'utf8')\n",
      "raw_text = f.read()\n",
      "print('{} characters read.'.format(len(raw_text)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "946006 characters read.\n"
       ]
      }
     ],
     "prompt_number": 25
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now that we have the data, let's clean it up some. This will involve several steps:\n",
      "\n",
      "1. Get rid of newlines;\n",
      "1. Case folding."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "clean_text = raw_text.replace('\\n', '').lower()\n",
      "print(clean_text[:75] + '...')"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "chapter iit was a sunday evening in october, and in common with many other ...\n"
       ]
      }
     ],
     "prompt_number": 26
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Finding Quotes"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we can start to identify the quoted quotes. We'll use a regular expression. Let's break it down part-by-part, though, first:\n",
      "\n",
      "* `ur` means that the code should be a `unicode` object, and it shouldn't try to escape any characters. This means that, for instance, \"\\n\" will be interpreted as two characters (backslash and \"n\") not a single newline.\n",
      "* `\u201c` looks for the first open quote.\n",
      "* `[^\u201d]+` matches any character *except* for a close quote. The plus sign means that it needs to find at least one non-close-quote character, but it will match as many as it can find.\n",
      "* `\u201d` finally matches the closing quote.\n",
      "\n",
      "All put together, this regular expression should match the quoted-quotes."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "matches = list(re.finditer(ur'\"[^\"]+\"', clean_text))\n",
      "print('{} quoted-quotes found.'.format(len(matches)))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "2897 quoted-quotes found.\n"
       ]
      }
     ],
     "prompt_number": 27
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Locations"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Let's see where these quotes are located. We'll essentially create a histogram of the starting locations of all of the matches. We'll process the data into [`numpy`](http://www.numpy.org/) arrays, and we'll use [`matplotlib`](http://matplotlib.org/) to draw the actual graph. (We'll closely follow the [histogram path example](http://matplotlib.org/examples/api/histogram_path_demo.html).\n",
      "\n",
      "For a more finely grained visualization, change the value of the second parameter to `np.histogram`."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "locations = [m.start() for m in matches]\n",
      "n, bins = np.histogram(locations, 100)\n",
      "\n",
      "fig, ax = plt.subplots()\n",
      "\n",
      "# corners of the rectangles\n",
      "left = np.array(bins[:-1])\n",
      "right = np.array(bins[1:])\n",
      "bottom = np.zeros(len(left))\n",
      "top = bottom + n\n",
      "\n",
      "XY = np.array([[left, left, right, right], [bottom, top, top, bottom]]).T\n",
      "\n",
      "barpath = path.Path.make_compound_path_from_polys(XY)\n",
      "\n",
      "patch = patches.PathPatch(barpath, facecolor='blue', edgecolor='gray', alpha=0.8)\n",
      "ax.add_patch(patch)\n",
      "\n",
      "ax.set_xlim(left[0], right[-1])\n",
      "ax.set_ylim(bottom.min(), top.max())\n",
      "\n",
      "plt.show()"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "display_data",
       "png": "iVBORw0KGgoAAAANSUhEUgAAAWwAAAD7CAYAAABOi672AAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
       "text": [
        "<matplotlib.figure.Figure at 0x10bc8ea10>"
       ]
      }
     ],
     "prompt_number": 28
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Tokenization"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "Now we have a corpus of the words. Let's break it up into tokens. We'll explicitly keep the punctuation."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def take_while(pred, input_str):\n",
      "    \"\"\"This returns the prefix of a string that matches pred,\n",
      "    and the suffix where the match stops.\"\"\"\n",
      "    for (i, c) in enumerate(input_str):\n",
      "        if not pred(c):\n",
      "            return (input_str[:i], input_str[i:])\n",
      "    else:\n",
      "        return (input_str, \"\")\n",
      "\n",
      "def is_punct(c):\n",
      "    \"\"\"Since `unicode` doesn't have a punctuation predicate...\"\"\"\n",
      "    return unicodedata.category(c)[0] == 'P'\n",
      "\n",
      "def tokenize(input_str):\n",
      "    \"\"\"This returns an iterator over the tokens in the string.\"\"\"\n",
      "    rest = None\n",
      "\n",
      "    # Since punctuations are always single characters, this isn't\n",
      "    # handled by `take_while`.\n",
      "    if is_punct(input_str[0]):\n",
      "        yield input_str[0]\n",
      "        rest = input_str[1:]\n",
      "\n",
      "    else:\n",
      "        # Try to match a string of letters or numbers. The first\n",
      "        # that succeeds, yield the token and stop trying.\n",
      "        for p in (unicode.isalpha, unicode.isdigit):\n",
      "            token, rest = take_while(p, input_str)\n",
      "            if token:\n",
      "                yield token\n",
      "                break\n",
      "        # If it wasn't a letter or number, skip a character.\n",
      "        else:\n",
      "            rest = input_str[1:]\n",
      "\n",
      "    # If there's more to try, get its tokenize and yield them.\n",
      "    if rest:\n",
      "        for token in tokenize(rest):\n",
      "            yield token\n",
      "\n",
      "print(list(tokenize(matches[0].group())))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "[u'\"', u'so', u'of', u'course', u',', u'\"']\n"
       ]
      }
     ],
     "prompt_number": 17
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Vector Space Model"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "A common way to deal with NLP documents is as a [vector space model](http://en.wikipedia.org/wiki/Vector_space_model). This takes the words out of order and just stores them as a list of frequencies. It also has to keep a look up table so you can go between words and vector indices easily.\n",
      "\n",
      "To make this easier, let's create a class to handle the lookup tables."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "class VectorSpace(object):\n",
      "    def __init__(self):\n",
      "        self.by_index = {}\n",
      "        self.by_token = {}\n",
      "    def __len__(self):\n",
      "        return len(self.by_index)\n",
      "    def get_index(self, token):\n",
      "        \"\"\"If it doesn't have an index for the token, create one.\"\"\"\n",
      "        try:\n",
      "            i = self.by_token[token]\n",
      "        except KeyError:\n",
      "            i = len(self.by_token)\n",
      "            self.by_token[token] = i\n",
      "            self.by_index[i] = token\n",
      "        return i\n",
      "    def lookup_token(self, i):\n",
      "        \"\"\"Returns None if there is no token at that position.\"\"\"\n",
      "        return self.by_index.get(i)\n",
      "    def lookup_index(self, token):\n",
      "        \"\"\"Returns None if there is no index for that token.\"\"\"\n",
      "        return self.by_token.get(token)\n",
      "    def vectorize(self, token_seq):\n",
      "        \"\"\"This turns a list of tokens into a numpy array.\"\"\"\n",
      "        v = [0] * len(self.by_token)\n",
      "        for token in token_seq:\n",
      "            i = self.get_index(token)\n",
      "            if i < len(v):\n",
      "                v[i] += 1\n",
      "            elif i == len(v):\n",
      "                v.append(1)\n",
      "            else:\n",
      "                raise Exception(\"Invalid index {} (len = {})\".format(i, len(v)))\n",
      "        return np.array(v)\n",
      "    def pad(self, array):\n",
      "        \"\"\"This pads a numpy array to match the dimensions of this vector space.\"\"\"\n",
      "        padding = np.zeros(len(self) - len(array))\n",
      "        return np.concatenate((array, padding))\n",
      "    \n",
      "vs = VectorSpace()\n",
      "\n",
      "corpus = [list(tokenize(m.group())) for m in matches]\n",
      "vs_corpus = [vs.vectorize(doc) for doc in corpus]\n",
      "vs_corpus = [vs.pad(d) for d in vs_corpus]\n",
      "\n",
      "vs_corpus[0][vs.lookup_index('the')]"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "metadata": {},
       "output_type": "pyout",
       "prompt_number": 18,
       "text": [
        "0.0"
       ]
      }
     ],
     "prompt_number": 18
    },
    {
     "cell_type": "heading",
     "level": 2,
     "metadata": {},
     "source": [
      "Frequencies"
     ]
    },
    {
     "cell_type": "markdown",
     "metadata": {},
     "source": [
      "We'll also want to be able to look at the frequencies of words. We'll filter out the punctuation for this, and then create a `Counter` of the fields."
     ]
    },
    {
     "cell_type": "code",
     "collapsed": false,
     "input": [
      "def frequencies(corpus):\n",
      "    \"\"\"This takes a list of list of tokens and returns a `Counter`.\"\"\"\n",
      "    return collections.Counter(\n",
      "        itertools.ifilter(lambda t: not (len(t) == 1 and is_punct(t)),\n",
      "                          itertools.chain.from_iterable(corpus)))\n",
      "\n",
      "freqs = frequencies(corpus)\n",
      "for (token, freq) in freqs.most_common(25):\n",
      "    print(u'{}\\t{}'.format(token, freq))"
     ],
     "language": "python",
     "metadata": {},
     "outputs": [
      {
       "output_type": "stream",
       "stream": "stdout",
       "text": [
        "i\t213\n",
        "the\t208\n",
        "you\t169\n",
        "to\t145\n",
        "s\t120\n",
        "it\t103\n",
        "a\t94\n",
        "of\t92\n",
        "that\t88\n",
        "is\t82\n",
        "t\t75\n",
        "and\t67\n",
        "in\t66\n",
        "he\t64\n",
        "what\t61\n",
        "but\t50\n",
        "one\t47\n",
        "my\t43\n",
        "for\t43\n",
        "there\t43\n",
        "are\t43\n",
        "me\t40\n",
        "have\t40\n",
        "mr\t39\n",
        "like\t39\n"
       ]
      }
     ],
     "prompt_number": 19
    }
   ],
   "metadata": {}
  }
 ]
}