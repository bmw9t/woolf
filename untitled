def is_punct(c):
    """Since `unicode` doesn't have a punctuation predicate..."""
    return unicodedata.category(c)[0] == 'P'


def get_unicode_category(unichars, prefix):
    """\
    This returns a generator over the unicode characters with a given major
    category.
    """
    return (c for c in unichars if unicodedata.category(c)[0] == prefix)


def make_token_re():
    unichars = [chr(c) for c in range(sys.maxunicode)]
    punct_chars = re.escape(''.join(get_unicode_category(unichars, 'P')))
    word_chars = re.escape(''.join(get_unicode_category(unichars, 'L')))
    number_chars = re.escape(''.join(get_unicode_category(unichars, 'N')))

    re_token = re.compile(r'''
            (?P<punct>  [{}]  ) |
            (?P<word>   [{}]+ ) |
            (?P<number> [{}]+ ) |
            (?P<trash>  .     )
        '''.format(punct_chars, word_chars, number_chars),
        re.VERBOSE,
        )
    return re_token


def tokenize(input_str, token_re=make_token_re()):
    """This returns an iterator over the tokens in the string."""
    return (
        m.group() for m in token_re.finditer(input_str) if not m.group('trash')
        )


class VectorSpace(object):
    """\
    This manages creating a vector space model of a corpus of documents. It
    makes sure that the indexes are consistent.

    Vectors of numpy arrays.
    """

    def __init__(self):
        self.by_index = {}
        self.by_token = {}

    def __len__(self):
        return len(self.by_index)

    def get_index(self, token):
        """If it doesn't have an index for the token, create one."""
        try:
            i = self.by_token[token]
        except KeyError:
            i = len(self.by_token)
            self.by_token[token] = i
            self.by_index[i] = token
        return i

    def lookup_token(self, i):
        """Returns None if there is no token at that position."""
        return self.by_index.get(i)

    def lookup_index(self, token):
        """Returns None if there is no index for that token."""
        return self.by_token.get(token)

    def vectorize(self, token_seq):
        """This turns a list of tokens into a numpy array."""
        v = [0] * len(self.by_token)
        for token in token_seq:
            i = self.get_index(token)
            if i < len(v):
                v[i] += 1
            elif i == len(v):
                v.append(1)
            else:
                raise Exception(
                    "Invalid index {} (len = {})".format(i, len(v)),
                    )
        return np.array(v)

    def get(self, vector, key):
        """This looks up the key in the vector given."""
        return vector[self.lookup_index(key)]

    def pad(self, array):
        """\
        This pads a numpy array to match the dimensions of this vector space.
        """
        padding = np.zeros(len(self) - len(array))
        return np.concatenate((array, padding))

    def vectorize_corpus(self, corpus):
        """\
        This converts a corpus (tokenized documents) into a collection of
        vectors.
        """
        vectors = [self.vectorize(doc) for doc in corpus]
        vectors = [self.pad(doc) for doc in vectors]
        return vectors


def frequencies(corpus):
    """This takes a list of tokens and returns a `Counter`."""
    return collections.Counter(
        itertools.ifilter(lambda t: not (len(t) == 1 and is_punct(t)),
                          itertools.chain.from_iterable(corpus)))


def find_quotes(doc, start_quote='“', end_quote='”'):
    """\
    This takes a tokenized document (with punctuation maintained) and returns
    tuple pairs of the beginning and ending indexes of the quoted quotes.
    """
    start = 0
    while start <= len(doc):
        try:
            start_quote_pos = doc.index(start_quote, start)
            end_quote_pos = doc.index(end_quote, start_quote_pos + 1)
        except ValueError:
            return
        yield (start_quote_pos, end_quote_pos + 1)
        start = end_quote_pos + 1


def tokenize_file(text):
    return list(tokenize(text))


def top_items(vectorizer, array, n=10):
    inv_vocab = dict((v, k) for (k, v) in vectorizer.vocabulary_.items())
    for row in array:
        indexes = list(enumerate(row))
        indexes.sort(key=operator.itemgetter(1), reverse=True)
        top = [(i, inv_vocab[i], c) for (i, c) in indexes[:n]]
        yield top


def vectorizer_report(title, klass, filenames, **kwargs):
    params = {
        'input': 'filename',
        'tokenizer': tokenize,
        'stop_words': 'english',
        }
    params.update(kwargs)
    v = klass(**params)
    corpus = v.fit_transform(filenames)
    a = corpus.toarray()

    print('# {}\n'.format(title))
    for (fn, top) in zip(filenames, top_items(v, a)):
        print('## {}\n'.format(fn))
        for row in top:
            print('{0[0]:>6}. {0[1]:<12}\t{0[2]:>5}'.format(row))
        print()


def main():
    # NOTE: before any processing you have to clean the text using clean_and_read_text().

    files = list(all_files(CORPUS))
    remove_short = lambda s: filter(lambda x: len(x) > 1, tokenize(s))
    vectorizer_report(
        'Raw Frequencies', CountVectorizer, files, tokenizer=remove_short,
        )
    vectorizer_report('Tf-Idf', TfidfVectorizer, files, tokenizer=remove_short)
    # for (root, _, files) in os.walk(CORPUS):
    #     for fn in files:
    #         text = clean_and_read_text(os.path.join(root, fn))
    #         quotations_check(text, fn)
if __name__ == '__main__':
    main()

# To do:

# Also make sure, once all the functions are written, that you don't have
# redundant cleaning of texts and looping through the corpus.

# It's currently preserving \s for every quote. Do we want to keep that?
# Presumably? It's going to throw off the percentages though.

# don situation

# have it clean up a text file that it reads in.
# automate it to run over the gutenberg corpus
